{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities as u\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch_geometric.data import Data, Dataset, InMemoryDataset, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, TopKPooling, GCNConv, BatchNorm\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
    "\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from math import floor\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, PATH):\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "def load_model(model_type, PATH, **kwargs):\n",
    "    model = model_type(**kwargs)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_test = False\n",
    "\n",
    "if dummy_test:\n",
    "    test_thm = '(fun (a A B) (a A (a A B)))'\n",
    "    print(test_thm)\n",
    "    thm = u.process_theorem(test_thm)\n",
    "    print(thm)\n",
    "    thm_tree, _ = u.thm_to_tree(thm)\n",
    "    print(len(thm_tree))\n",
    "    print(thm_tree.subtrees[0].parents[0])\n",
    "    thm_tree = u.merge_subexpressions(thm_tree)\n",
    "    x = u.graph_to_data(thm_tree)\n",
    "    print(x)\n",
    "\n",
    "    #print([t.root for t in thm_tree.subtrees[0].subtrees])\n",
    "\n",
    "\n",
    "    print(thm_tree.root)\n",
    "    print([t.root for t in thm_tree.subtrees])\n",
    "    t_0, t_1 = thm_tree.subtrees\n",
    "    print([t.root for t in t_0.subtrees])\n",
    "    print([t.root for t in t_1.subtrees])\n",
    "    print(t_1.subtrees[0].subtree_str)\n",
    "    print(len(thm_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopLevelProofDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    InMemoryDataset, collects training examples from the first 150 files\n",
    "    \"\"\"\n",
    "    def __init__(self, root='', transform=None, pre_transform=None):\n",
    "        super(TopLevelProofDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'../datasets/{dataset_name}.dataset']\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        global data\n",
    "        data_list = []\n",
    "        all_features = set()\n",
    "        trees = []\n",
    "        \n",
    "        for thm, y in tqdm(data):\n",
    "            thm = u.process_theorem(thm)\n",
    "            tree, distinct_features = u.thm_to_tree(thm, to_merge)\n",
    "            all_features = all_features | distinct_features\n",
    "            trees.append((tree, y))\n",
    "        \n",
    "#         normalized_features = {k: [random.random() for i in range(128)] for k in list(all_features)}\n",
    "        normalized_features = {k: [i] for i,k in enumerate(all_features)}\n",
    "            \n",
    "        for idx, (tree, y) in tqdm(enumerate(trees)):\n",
    "            merged_tree = u.merge_subexpressions(tree) if to_merge else tree\n",
    "            x, (edge_index_up, edge_index_down), (edge_features_up, edge_features_down) = u.graph_to_data(tree, \n",
    "                                                                                                           normalized_features)\n",
    "            datum = Data(x=x, \n",
    "                        y=y, \n",
    "                        edge_index=torch.cat((edge_index_up, edge_index_down), dim=1),\n",
    "                        edge_attr=torch.cat((edge_features_up, edge_features_down)),\n",
    "                       )\n",
    "            data_list.append(datum)\n",
    "#             trees[idx] = None\n",
    "            \n",
    "        \n",
    "        all_data, slices = self.collate(data_list)\n",
    "        torch.save((all_data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_graphs = 10\n",
    "n_files = 600\n",
    "\n",
    "class ProofDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Saved dataset. Collects a fixed number of training examples, and saves each in an individual file in \n",
    "    'datasets/{dataset_name}/    \n",
    "    \"\"\"\n",
    "    def __init__(self, root='', transform=None, pre_transform=None):\n",
    "        super(ProofDataset, self).__init__(root, transform, pre_transform)\n",
    "#         self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'{dataset_name}/data_{i}.pt' for i in range(n_graphs)]\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        global seen\n",
    "        global data\n",
    "        global binary\n",
    "        global to_merge\n",
    "        print(f'{dataset_name}: binary={bool(binary)}, merge={to_merge}')\n",
    "        \n",
    "        \n",
    "#         seen = set()\n",
    "#         counter = dict()\n",
    "        all_features = set()\n",
    "        \n",
    "        trees = []\n",
    "        data = []\n",
    "        count = 0\n",
    "        collecting = True\n",
    "        data_dict = dict()\n",
    "        \n",
    "        if not os.path.exists(f'processed/{dataset_name}'):\n",
    "            os.makedirs(f'processed/{dataset_name}')\n",
    "        \n",
    "        for i in range(n_files):\n",
    "            if collecting is False:\n",
    "                break\n",
    "                \n",
    "\n",
    "            data_from_file = u.get_data_from_file(i, binary, only_top=False)\n",
    "        \n",
    "            for thm, y in data_from_file:\n",
    "                if collecting is False:\n",
    "                    break\n",
    "                    \n",
    "                if thm in data_dict.keys():\n",
    "                    data_dict[thm] = min(data_dict[thm], y)\n",
    "                else:\n",
    "                    data_dict[thm] = y\n",
    "                    count += 1\n",
    "                    \n",
    "                    if count == n_graphs:\n",
    "                        collecting = False\n",
    "                    \n",
    "                    if count % 100 == 99:\n",
    "                        print(f'{count + 1}/{n_graphs} : {((count + 1)/n_graphs) *100:.2f}%', end='\\r', flush=True)\n",
    "            \n",
    "            \n",
    "        data = [(thm, y) for t,y in data_dict.items()]\n",
    "        \n",
    "        \n",
    "        for thm, y in tqdm(data):\n",
    "            thm = u.process_theorem(thm)\n",
    "            tree, distinct_features = u.thm_to_tree(thm, to_merge)\n",
    "            all_features = all_features | distinct_features\n",
    "            trees.append((tree, y))\n",
    " \n",
    "    #         normalized_features = {k: [random.random() for i in range(128)] for k in list(all_features)}\n",
    "        print()\n",
    "        normalized_features = {feature: [i] for i,feature in enumerate(all_features)}\n",
    "\n",
    "        for idx, (tree, y) in tqdm(enumerate(trees)):\n",
    "            merged_tree = u.merge_subexpressions(tree) if to_merge else tree\n",
    "            x, (edge_index_up, edge_index_down), (edge_features_up, edge_features_down) = u.graph_to_data(tree, \n",
    "                                                                                                           normalized_features)\n",
    "            datum = Data(x=x, \n",
    "                        y=y, \n",
    "                        edge_index=torch.cat((edge_index_up, edge_index_down), dim=1),\n",
    "                        edge_attr=torch.cat((edge_features_up, edge_features_down)),\n",
    "                       )\n",
    "            torch.save(datum, f'processed/{dataset_name}/data_{idx}.pt')\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    \n",
    "    def get(self, idx):\n",
    "        datum = torch.load(f'processed/{dataset_name}/data_{idx}.pt')\n",
    "        return datum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 (Subgraph Pooling Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Standard MLP scheme for use in Subgraph Pooling model\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dim=0):\n",
    "        super(MLP, self).__init__()\n",
    "        self.lin1 = Linear(in_channels, 64, dim)\n",
    "#         self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.hidden = Linear(64, 32)\n",
    "#         self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.lin2 = Linear(32, 32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.lin1(x))\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.elu(self.hidden(x))\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.elu(self.lin2(x))\n",
    "        return x\n",
    "\n",
    "    \n",
    "class PaliwalMP(MessagePassing):\n",
    "    \"\"\"Define the message-passing scheme from Subgraph Pooling paper.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PaliwalMP, self).__init__(aggr='mean', flow='source_to_target') #  \"Mean\" aggregation.\n",
    "        \n",
    "        # MLP for Parents and Children, step 2 of Paliwal MP\n",
    "        self.MLP_edge = BuildingBlock(3*in_channels, in_channels)\n",
    "        self.MLP_edge_hat = BuildingBlock(3*in_channels, in_channels)\n",
    "        \n",
    "        # MLP to pass aggregated message through, step 3 of Paliwal MP\n",
    "        self.MLP_aggr = BuildingBlock(3*in_channels, in_channels)\n",
    "\n",
    "        \n",
    "    def forward(self, x, edge_index_parents, edge_index_children, edge_attr_parents, edge_attr_children):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index_x has shape [2, E/2]\n",
    "        out_parents = self.propagate(edge_index_parents, \n",
    "                                     x=x, \n",
    "                                     edge_attr=edge_attr_parents, \n",
    "                                     direction='up', \n",
    "                                     size=None)\n",
    "        out_children = self.propagate(edge_index_children, \n",
    "                                      x=x, \n",
    "                                      edge_attr=edge_attr_children, \n",
    "                                      direction='down', \n",
    "                                      size=None)\n",
    "        \n",
    "        out = torch.cat([x, out_parents, out_children], dim=1)\n",
    "        out = self.MLP_aggr(out) + x\n",
    "        \n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "    def message(self, x_i, x_j, edge_attr, direction):\n",
    "\n",
    "        s_ij = torch.cat([x_i, x_j, edge_attr], dim=1)\n",
    "        if direction == 'up':\n",
    "            s_ij = self.MLP_edge(s_ij)\n",
    "        elif direction == 'down':\n",
    "            s_ij = self.MLP_edge_hat(s_ij)\n",
    "        \n",
    "        return s_ij\n",
    "\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "\n",
    "class BuildingBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Standard MLP scheme for use in Subgraph Pooling model\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):#, dim=0):\n",
    "        super(BuildingBlock, self).__init__()\n",
    "        self.lin1 = Linear(in_channels, 256)\n",
    "        self.hidden = Linear(256, 128)\n",
    "        self.lin2 = Linear(128, 128)\n",
    "        \n",
    "#         self.bn1 = nn.BatchNorm1d(256)\n",
    "#         self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.lin1(x))\n",
    "        x = F.dropout(x, 0.3)\n",
    "#         x = self.bn1(x)\n",
    "        x = F.elu(self.hidden(x))\n",
    "        x = F.dropout(x, 0.3)\n",
    "#         x = self.bn2(x)\n",
    "        x = F.elu(self.lin2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class PaliwalNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implement GNN from Subgraph Pooling model. Accepts an arbitrary-size graph and produces a scalar output value.\n",
    "    \"\"\"\n",
    "    def __init__(self, t, no_upsample=False, sigmoid=True, softmax=False):\n",
    "        super(PaliwalNet, self).__init__()\n",
    "        \n",
    "        self.no_upsample = no_upsample\n",
    "        self.using_softmax = softmax\n",
    "        \n",
    "        self.embedding = Embedding(num_embeddings=distinct_features[dataset_name]+1, embedding_dim=embed_dim)\n",
    "        \n",
    "        self.MLP_V = BuildingBlock(128, 128)\n",
    "        if t > 0:\n",
    "            self.MLP_E = BuildingBlock(1, 128)\n",
    "        \n",
    "        self.message_passing_steps = nn.ModuleList()\n",
    "        for i in range(t):\n",
    "            self.message_passing_steps.append(PaliwalMP(embed_dim, embed_dim))\n",
    "            \n",
    "\n",
    "        self.conv1 = nn.Conv1d(128, 512, 1)\n",
    "        self.conv2 = nn.Conv1d(512, 1024, 1)\n",
    "\n",
    "        # TODO: Try removing some layers or adding batch norm\n",
    "        self.lin1 = Linear(1024, 512)\n",
    "        self.lin2 = Linear(512, 256)\n",
    "        self.lin3 = Linear(256, 128)\n",
    "        self.lin4 = Linear(128, 128)\n",
    "        if self.using_softmax:\n",
    "            self.lin5 = Linear(128, 4)\n",
    "        else:\n",
    "            self.lin5 = Linear(128, 1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        if sigmoid:\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.using_sigmoid = True\n",
    "        else:\n",
    "            self.using_sigmoid = False\n",
    "        if softmax:\n",
    "            self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "  \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        edge_index_u, edge_index_d = torch.split(edge_index, int(edge_index.shape[1]/2), dim=1)\n",
    "        edge_attr_u, edge_attr_d = torch.split(edge_attr, int(edge_attr.shape[0]/2))\n",
    "        \n",
    "        # Generate learnable embeddings for node features\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.embedding(x)\n",
    "        x = self.MLP_V(x)\n",
    "        \n",
    "        # Embed node and edge features into high dimensional space\n",
    "        if len(self.message_passing_steps) > 0:\n",
    "            edge_attr_u = self.MLP_E(edge_attr_u.float())\n",
    "            edge_attr_d = self.MLP_E(edge_attr_d.float())\n",
    "        \n",
    "        for i, message_passing_step in enumerate(self.message_passing_steps):\n",
    "            x = message_passing_step(x, edge_index_u, edge_index_d, edge_attr_u, edge_attr_d)\n",
    "        \n",
    "        x = x.transpose(0,1).unsqueeze(0)\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = x.squeeze(0).transpose(0,1)\n",
    "        \n",
    "        # Final prediction network\n",
    "        x = gmp(x, batch)\n",
    "        \n",
    "        x = F.elu(self.lin1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.elu(self.lin2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.elu(self.lin3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = F.elu(self.lin4(x))\n",
    "        x = self.bn4(x)\n",
    "        if self.using_sigmoid:\n",
    "            x = self.sigmoid(self.lin5(x))\n",
    "        elif self.using_softmax:\n",
    "            x = self.softmax(self.lin5(x))\n",
    "        else:\n",
    "            x = self.lin5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_distribution(data):\n",
    "    \"\"\"Returns the distribution of target values for a dataset\"\"\"\n",
    "    counter = dict()\n",
    "    for _, y in data:\n",
    "        if y in counter:\n",
    "            counter[y] += 1\n",
    "        else:\n",
    "            counter[y] = 1\n",
    "    counter = list(counter.items())\n",
    "    counter.sort(key=lambda x: x[0], reverse=False)\n",
    "    percentages = [(x, y/len(data)*100) for x,y in counter]\n",
    "    return percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_distinct_features(data):\n",
    "    \"\"\"Returns the number of distinct node values for a dataset\"\"\"\n",
    "    distinct_features = set()\n",
    "    for thm, _ in tqdm(data):\n",
    "        thm = u.process_theorem(thm)\n",
    "        thm_tree, features = u.thm_to_tree(thm, to_merge=False)\n",
    "        distinct_features = distinct_features.union(features)\n",
    "    return len(distinct_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, epoch, crit, optimizer, device, len_dataset):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        data = data.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "#         label = torch.unsqueeze(data.y.to(device), 1).float()\n",
    "        label = data.y.to(device).long()\n",
    "        \n",
    "        loss = crit(output, label)\n",
    "        loss.backward()\n",
    "        loss = loss.detach()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    wandb.log({\"Train Loss\": loss_all/len_dataset},\n",
    "             step=epoch+1)\n",
    "    \n",
    "    \n",
    "    return loss_all / len(train_dataset), 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, epoch, crit, device, len_dataset):\n",
    "    model.eval()\n",
    "    loss_all = 0\n",
    "    all_preds = None\n",
    "    all_labels = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "#             label = torch.unsqueeze(data.y.to(device), 1).float()\n",
    "            label = data.y.to(device).long()\n",
    "            \n",
    "            loss = crit(output, label)\n",
    "            loss = loss.detach()\n",
    "            loss_all += data.num_graphs * loss.item()\n",
    "            \n",
    "            if all_labels is not None:\n",
    "                all_labels = torch.cat([all_labels, label.detach().cpu()])\n",
    "            else:\n",
    "                all_labels = label.detach().cpu()\n",
    "            \n",
    "            _, output = torch.max(output, dim=1)\n",
    "            if all_preds is not None:\n",
    "                all_preds = torch.cat([all_preds, output.detach().cpu()])\n",
    "            else:\n",
    "                all_preds = output.detach().cpu()\n",
    "    \n",
    "\n",
    "    all_labels, all_preds = all_labels.numpy(), all_preds.numpy()\n",
    "    wandb.log({\"Test Loss\": loss_all/len_dataset}, \n",
    "              step=epoch+1)\n",
    "    \n",
    "    return loss_all / len(valid_dataset), 0, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New general 'train_model' function which handles full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset,\n",
    "                valid_dataset,\n",
    "                crit,\n",
    "                experiment_label):\n",
    "    \n",
    "\n",
    "    config = wandb.config\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              pin_memory=True,\n",
    "                              batch_size=config.batch_size, \n",
    "                              shuffle=config.shuffle_data,\n",
    "                              num_workers=config.n_workers\n",
    "                             )\n",
    "    \n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              pin_memory=True,\n",
    "                              batch_size=config.batch_size, \n",
    "                              shuffle=config.shuffle_data,\n",
    "                              num_workers=config.n_workers,\n",
    "                             )\n",
    "    \n",
    "    if not os.path.exists(f'plotting/{experiment_label}'):\n",
    "        os.makedirs(f'plotting/{experiment_label}')\n",
    "        print('Made it')\n",
    "    \n",
    "\n",
    "    if config.model == 'PaliwalNet':\n",
    "        if 'M' in config.dataset:\n",
    "            model = PaliwalNet(t=config.message_passing_steps, sigmoid=False, softmax=True)\n",
    "        else:\n",
    "            model = PaliwalNet(t=config.message_passing_steps)\n",
    "    model = model.to(config.device)\n",
    "    \n",
    "    \n",
    "    if config.optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum)\n",
    "    elif config.optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "        \n",
    "    wandb.watch(model, log=\"all\")\n",
    "    \n",
    "    \n",
    "    # Run initial pass through validation loop\n",
    "    valid_loss, valid_acc, preds, labels = test(model, valid_loader, 0, crit, config.device, len(valid_dataset))\n",
    "    best_loss = valid_loss\n",
    "    best_loss_epoch = 1\n",
    "    wandb.log({\"Best Loss\": best_loss},\n",
    "              step=1)\n",
    "    \n",
    "    df = pd.DataFrame(data={'Predictions': preds, 'Labels': labels})\n",
    "    df['epoch'] = 0\n",
    "    df.to_csv(f'plotting/{experiment_label}/{experiment_label}_epoch_0')\n",
    "    \n",
    "#     save_model(model, f'models/{experiment_label}_best_epoch_0')\n",
    "    valid_losses, valid_accuracies = [valid_loss], [valid_acc]\n",
    "    train_losses, train_accuracies = [], []\n",
    "    \n",
    "    # Train for n_epochs\n",
    "    for epoch in tqdm(range(config.n_epochs)):\n",
    "        epoch_loss, epoch_acc = train(model, train_loader, epoch, crit, optimizer, config.device, len(train_dataset))\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        # Every 10 epochs, run through a validation loop\n",
    "        if epoch % 5 == 4:\n",
    "            valid_loss, valid_acc, preds, labels = test(model, valid_loader, epoch, crit, config.device, len(valid_dataset))\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_accuracies.append(valid_acc)\n",
    "            \n",
    "            # Record the highest observed validation accuracy\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                best_loss_epoch = epoch + 1\n",
    "                \n",
    "            wandb.log({\"Best Loss\": best_loss},\n",
    "                      step=epoch+1)\n",
    "                \n",
    "\n",
    "            df = pd.DataFrame(data={'Predictions': preds, 'Labels': labels})\n",
    "            df['epoch'] = epoch+1\n",
    "            df.to_csv(f'plotting/{experiment_label}/{experiment_label}_epoch_{epoch+1}')\n",
    "    \n",
    "    \n",
    "    # Output loss/acc stats to csv\n",
    "    save_model(model, f'models/{experiment_label}_final_{config.n_epochs}')\n",
    "    wandb.save(f'models/{experiment_label}_final_{config.n_epochs}')\n",
    "    \n",
    "    validation_stats = np.array([valid_losses, valid_accuracies])\n",
    "    training_stats = np.array([train_losses, train_accuracies])\n",
    "    np.savetxt(f'stats/{experiment_label}_validation_stats.csv', validation_stats, delimiter=',')\n",
    "    np.savetxt(f'stats/{experiment_label}_training_stats.csv', training_stats, delimiter=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets: Binary/Multiclass (B/M), OnlyTop/Subtheorems (O/S), Merged/Unmerged (m/u)\n",
    "dataset_features = {'n_classes': ['M', 'B'],\n",
    "                   'theorems_used': ['S', 'O'],\n",
    "                   'subexpression_sharing': ['u', 'm']}\n",
    "\n",
    "distinct_features = dict()\n",
    "distinct_features = pickle.load(open('distinct_features.p', 'rb'))\n",
    "\n",
    "data = None\n",
    "dataset_name = None\n",
    "to_merge = None\n",
    "binary = None\n",
    "only_top = None\n",
    "n_graphs = 30\n",
    "\n",
    "seen = None\n",
    "\n",
    "\n",
    "# def create_datasets():\n",
    "#     global data\n",
    "#     global dataset_name\n",
    "#     global to_merge\n",
    "#     global binary\n",
    "    \n",
    "#     for binary, x in enumerate(dataset_features['n_classes']):\n",
    "#         for merged, z in enumerate(dataset_features['subexpression_sharing']):\n",
    "#             dataset_name = x + z\n",
    "# #                 data = u.make_data(binary=bool(binary), only_top=bool(only_top))\n",
    "#             to_merge = bool(merged)\n",
    "#             dataset = ProofDataset()\n",
    "#             distinct_features[dataset_name] = get_num_distinct_features(data)\n",
    "#             data_distribution = get_data_distribution(data)\n",
    "#             print(f'{dataset_name}({len(data)}): ', data_distribution)\n",
    "            \n",
    "\n",
    "def create_datasets():\n",
    "    global data\n",
    "    global dataset_name\n",
    "    global to_merge\n",
    "    global binary\n",
    "    \n",
    "    for binary, x in enumerate(dataset_features['n_classes']):\n",
    "        binary = 0\n",
    "        x = 'M2'\n",
    "        for merged, z in enumerate(dataset_features['subexpression_sharing']):\n",
    "            dataset_name = x + z\n",
    "            data = u.make_data(binary=bool(binary), only_top=False)\n",
    "            data = list(set(data))\n",
    "            data_dict = dict()\n",
    "            for t,y in data:\n",
    "                if t in data_dict.keys():\n",
    "                    data_dict[t] = min(data_dict[t], y)\n",
    "                else:\n",
    "                    data_dict[t] = y\n",
    "            \n",
    "            data = [(t,y) for t,y in data_dict.items()]\n",
    "            if binary == 1:\n",
    "                data_0 = [(t,y) for t,y in data if y == 0]\n",
    "                data_1 = [(t,y) for t,y in data if y == 1]\n",
    "                min_len = min(len(data_0), len(data_1))\n",
    "                data_0 = [(t,y) for t,y in data_0[:min_len]]\n",
    "                data_1 = [(t,y) for t,y in data_1[:min_len]]\n",
    "                data = data_0 + data_1\n",
    "            else:\n",
    "                temp_data = []\n",
    "                for i in range(4):\n",
    "                    temp_data.append([(t,y) for t,y in data if y == i])\n",
    "                min_len = min([len(temp_data[j]) for j in range(4)])\n",
    "                for i in range(4):\n",
    "                    temp_data[i] = temp_data[i][:min_len]\n",
    "                data = temp_data[0] + temp_data[1] + temp_data[2] + temp_data[3]\n",
    "            \n",
    "            data_distribution = get_data_distribution(data)\n",
    "            to_merge = bool(merged)\n",
    "            distinct_features[dataset_name] = get_num_distinct_features(data)\n",
    "            print(data_distribution)\n",
    "            dataset = TopLevelProofDataset()\n",
    "\n",
    "            \n",
    "            print(f'{dataset_name}({len(data)}): ', data_distribution)\n",
    "        break\n",
    "        \n",
    "\n",
    "# create_datasets()\n",
    "# pickle.dump(distinct_features, open( 'distinct_features.p', 'wb' ))\n",
    "distinct_features = pickle.load(open('distinct_features.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'M2u'\n",
    "experiment_label = 'M2u0_ce'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps': 0,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 101,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.8,\n",
    "            'weight_decay': 0,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=nn.CrossEntropyLoss(), experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'M2u'\n",
    "experiment_label = 'M2u4_ce'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps': 4,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 101,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.8,\n",
    "            'weight_decay': 0,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=nn.CrossEntropyLoss(), experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'M2m'\n",
    "experiment_label = 'M2m0'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps': 0,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 101,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.8,\n",
    "            'weight_decay': 0,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=F.mse_loss, experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'M2u'\n",
    "experiment_label = 'M2u1'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps':1,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 101,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.8,\n",
    "            'weight_decay': 0,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=F.mse_loss, experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'M2u'\n",
    "experiment_label = 'M2u2'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps':2,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 101,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.8,\n",
    "            'weight_decay': 0,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=F.mse_loss, experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'M2u'\n",
    "experiment_label = 'M2u3'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps': 3,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 101,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.8,\n",
    "            'weight_decay': 0,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=F.mse_loss, experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'M2u'\n",
    "experiment_label = 'M2u4'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps': 4,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 101,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.8,\n",
    "            'weight_decay': 0,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=F.mse_loss, experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'M2m'\n",
    "experiment_label = 'M2m1'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps': 1,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 101,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.8,\n",
    "            'weight_decay': 0,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=F.mse_loss, experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'M2m'\n",
    "experiment_label = 'M2m2'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps': 2,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 101,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.8,\n",
    "            'weight_decay': 0,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=F.mse_loss, experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'M2m'\n",
    "experiment_label = 'M2m3'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps': 3,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 101,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.8,\n",
    "            'weight_decay': 0,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=F.mse_loss, experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'M2m'\n",
    "experiment_label = 'M2m4'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps': 4,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 101,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.8,\n",
    "            'weight_decay': 0,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=F.mse_loss, experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Bu'\n",
    "experiment_label = 'Bu12_2'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps': 12,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 250,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr': 0.005,\n",
    "            'momentum': 0.8,\n",
    "            'weight_decay': None,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "# wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "# train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=F.mse_loss, experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Bm'\n",
    "experiment_label = 'Bm1_3'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps': 1,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 101,\n",
    "            'optimizer': 'Adam',\n",
    "            'lr': 0.001,\n",
    "            'momentum': None,\n",
    "            'weight_decay': 0,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=F.mse_loss, experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Bm'\n",
    "experiment_label = 'Bm4_3'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps': 4,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 101,\n",
    "            'optimizer': 'Adam',\n",
    "            'lr': 0.001,\n",
    "            'momentum': None,\n",
    "            'weight_decay': 0,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=F.mse_loss, experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Bm'\n",
    "experiment_label = 'Bm12_2'\n",
    "\n",
    "config = {\n",
    "            'model': 'PaliwalNet',\n",
    "            'message_passing_steps': 12,\n",
    "            'batch_size': 16,\n",
    "            'n_workers': 4,\n",
    "            'n_epochs': 250,\n",
    "            'optimizer': 'SGD',\n",
    "            'lr': 0.005,\n",
    "            'momentum': 0.8,\n",
    "            'weight_decay': None,\n",
    "            'SWA': False,\n",
    "            'device': 'cuda:1',\n",
    "            'seed': 42,\n",
    "            'dataset': dataset_name,\n",
    "            'shuffle_data': True\n",
    "        }\n",
    "\n",
    "# wandb.init(name=experiment_label, project=\"complexity-prediction\", config=config)\n",
    "\n",
    "dataset = TopLevelProofDataset().shuffle()\n",
    "\n",
    "train_dataset = dataset[:2*floor(len(dataset)/3)]\n",
    "valid_dataset = dataset[2*floor(len(dataset)/3):]\n",
    "print(f'Training Data: {len(train_dataset)}, Validation Data: {len(valid_dataset)}')\n",
    "\n",
    "# train_model(train_dataset=train_dataset, valid_dataset=valid_dataset, crit=F.mse_loss, experiment_label=experiment_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
